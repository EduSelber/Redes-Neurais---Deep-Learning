{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c43b5aa8",
   "metadata": {},
   "source": [
    "### Excercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9ade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass:\n",
      "z1 = [ 0.27 -0.18]\n",
      "h1 = tanh(z1) = [ 0.26362484 -0.17808087]\n",
      "u2 = 0.38523667817130075\n",
      "yhat = tanh(u2) = 0.36724656264510797\n",
      "Loss L = 0.4003769124844312\n",
      "\n",
      "Backward pass (gradients):\n",
      "dL/dyhat = -1.265506874709784\n",
      "dL/du2   = -1.0948279147135995\n",
      "dL/dW2   = [-0.28862383  0.19496791]\n",
      "dL/db2   = -1.0948279147135995\n",
      "dL/dh1   = [-0.54741396  0.32844837]\n",
      "dL/dz1   = [-0.50936975  0.31803236]\n",
      "dL/dW1   =\n",
      " [[-0.25468488  0.10187395]\n",
      " [ 0.15901618 -0.06360647]]\n",
      "dL/db1   = [-0.50936975  0.31803236]\n",
      "\n",
      "Updated parameters (eta = 0.1):\n",
      "W2_new = [ 0.52886238 -0.31949679]\n",
      "b2_new = 0.30948279147136\n",
      "W1_new =\n",
      " [[ 0.32546849 -0.1101874 ]\n",
      " [ 0.18409838  0.40636065]]\n",
      "b1_new = [ 0.15093698 -0.23180324]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.array([0.5, -0.2], dtype=float)\n",
    "y = 1.0\n",
    "\n",
    "W1 = np.array([[0.3, -0.1],\n",
    "               [0.2,  0.4]], dtype=float)\n",
    "b1 = np.array([ 0.1, -0.2], dtype=float)\n",
    "\n",
    "W2 = np.array([0.5, -0.3], dtype=float)  \n",
    "b2 = 0.2\n",
    "eta_update = 0.1\n",
    "\n",
    "\n",
    "z1 = W1 @ x + b1\n",
    "h1 = np.tanh(z1)\n",
    "u2 = W2 @ h1 + b2\n",
    "yhat = np.tanh(u2)\n",
    "\n",
    "\n",
    "L = (y - yhat)**2\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(\"z1 =\", z1)\n",
    "print(\"h1 = tanh(z1) =\", h1)\n",
    "print(\"u2 =\", u2)\n",
    "print(\"yhat = tanh(u2) =\", yhat)\n",
    "print(\"Loss L =\", L)\n",
    "\n",
    "\n",
    "dL_dyhat = 2*(yhat - y)               \n",
    "d_tanh_u2 = 1 - np.tanh(u2)**2\n",
    "dL_du2 = dL_dyhat * d_tanh_u2\n",
    "\n",
    "\n",
    "dL_dW2 = dL_du2 * h1                 \n",
    "dL_db2 = dL_du2\n",
    "\n",
    "\n",
    "dL_dh1 = dL_du2 * W2\n",
    "d_tanh_z1 = 1 - np.tanh(z1)**2\n",
    "dL_dz1 = dL_dh1 * d_tanh_z1            \n",
    "dL_dW1 = np.outer(dL_dz1, x)\n",
    "dL_db1 = dL_dz1\n",
    "\n",
    "print(\"\\nBackward pass (gradients):\")\n",
    "print(\"dL/dyhat =\", dL_dyhat)\n",
    "print(\"dL/du2   =\", dL_du2)\n",
    "print(\"dL/dW2   =\", dL_dW2)\n",
    "print(\"dL/db2   =\", dL_db2)\n",
    "print(\"dL/dh1   =\", dL_dh1)\n",
    "print(\"dL/dz1   =\", dL_dz1)\n",
    "print(\"dL/dW1   =\\n\", dL_dW1)\n",
    "print(\"dL/db1   =\", dL_db1)\n",
    "\n",
    "\n",
    "W2_new = W2 - eta_update * dL_dW2\n",
    "b2_new = b2 - eta_update * dL_db2\n",
    "W1_new = W1 - eta_update * dL_dW1\n",
    "b1_new = b1 - eta_update * dL_db1\n",
    "\n",
    "print(\"\\nUpdated parameters (eta = 0.1):\")\n",
    "print(\"W2_new =\", W2_new)\n",
    "print(\"b2_new =\", b2_new)\n",
    "print(\"W1_new =\\n\", W1_new)\n",
    "print(\"b1_new =\", b1_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb215a",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea4728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1 | loss 0.4612\n",
      "epoch  50 | loss 0.2585\n",
      "epoch 100 | loss 0.1581\n",
      "epoch 150 | loss 0.2104\n",
      "epoch 200 | loss 0.1864\n",
      "epoch 250 | loss 0.2356\n",
      "epoch 300 | loss 0.1400\n",
      "\n",
      "Test accuracy (binary): 0.890\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "n_total = 1000\n",
    "X0, y0 = make_classification(\n",
    "    n_samples=n_total//2, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, n_classes=2, class_sep=1.3, flip_y=0.0, random_state=7\n",
    ")\n",
    "y0 = (y0==0).astype(int) \n",
    "\n",
    "X1a, y1a = make_classification(\n",
    "    n_samples=n_total//4, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, n_classes=2, class_sep=1.3, flip_y=0.0, random_state=8\n",
    ")\n",
    "X1b, y1b = make_classification(\n",
    "    n_samples=n_total - (n_total//2 + n_total//4), n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, n_classes=2, class_sep=1.3, flip_y=0.0, random_state=9\n",
    ")\n",
    "y1a[:] = 1\n",
    "y1b[:] = 1\n",
    "\n",
    "X = np.vstack([X0, X1a, X1b]).astype(float)\n",
    "y = np.hstack([y0, y1a, y1b]).astype(int)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "in_features = 2\n",
    "h = 32\n",
    "\n",
    "W1 = rng.normal(0, 1, (h, in_features)) / np.sqrt(in_features)\n",
    "b1 = np.zeros((h,))\n",
    "W2 = rng.normal(0, 1, (1, h)) / np.sqrt(h)\n",
    "b2 = np.zeros((1,))\n",
    "\n",
    "lr = 5e-2\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "ytr_ = ytr.reshape(-1, 1).astype(float)\n",
    "N = Xtr.shape[0]\n",
    "eps = 1e-12\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    idx = rng.permutation(N)\n",
    "    Xs = Xtr[idx]; ys = ytr_[idx]\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        xb = Xs[i:i+batch_size]\n",
    "        yb = ys[i:i+batch_size]\n",
    "\n",
    "      \n",
    "        z1 = xb @ W1.T + b1\n",
    "        h1 = np.tanh(z1)\n",
    "\n",
    "        u2 = h1 @ W2.T + b2 \n",
    "        yhat = 1.0 / (1.0 + np.exp(-u2))\n",
    "\n",
    "        yclip = np.clip(yhat, eps, 1-eps)\n",
    "        loss = -np.mean(yb*np.log(yclip) + (1-yb)*np.log(1-yclip))\n",
    "\n",
    "    \n",
    "        B = yb.shape[0]\n",
    "        dL_dyhat = (yhat - yb) / (yclip*(1-yclip)) / B\n",
    "        dL_du2 = dL_dyhat * (yhat*(1-yhat))  \n",
    "\n",
    "        dL_dW2 = dL_du2.T @ h1\n",
    "        dL_db2 = dL_du2.sum(axis=0)\n",
    "\n",
    "        dL_dh1 = dL_du2 @ W2\n",
    "        dL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n",
    "\n",
    "        dL_dW1 = dL_dz1.T @ xb\n",
    "        dL_db1 = dL_dz1.sum(axis=0)\n",
    "\n",
    "\n",
    "        W2 -= lr * dL_dW2\n",
    "        b2 -= lr * dL_db2\n",
    "        W1 -= lr * dL_dW1\n",
    "        b1 -= lr * dL_db1\n",
    "\n",
    "    if ep % 50 == 0 or ep == 1:\n",
    "        print(f\"epoch {ep:3d} | loss {loss:.4f}\")\n",
    "\n",
    "\n",
    "z1 = Xte @ W1.T + b1\n",
    "h1 = np.tanh(z1)\n",
    "u2 = h1 @ W2.T + b2\n",
    "yhat = 1.0 / (1.0 + np.exp(-u2))\n",
    "ypred = (yhat.ravel() >= 0.5).astype(int)\n",
    "acc = (ypred == yte).mean()\n",
    "print(f\"\\nTest accuracy (binary): {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df584f34",
   "metadata": {},
   "source": [
    "### Excercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1 | loss 0.8970\n",
      "epoch  50 | loss 0.5367\n",
      "epoch 100 | loss 0.3963\n",
      "epoch 150 | loss 0.3406\n",
      "epoch 200 | loss 0.3817\n",
      "epoch 250 | loss 0.3475\n",
      "epoch 300 | loss 0.2155\n",
      "\n",
      "Test accuracy (3 classes): 0.817\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "n_total = 1500\n",
    "n_per = n_total // 3\n",
    "\n",
    "\n",
    "sizes0 = [n_per//2, n_per - n_per//2]\n",
    "X0_parts = []\n",
    "for k, ns in enumerate(sizes0):\n",
    "    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n",
    "                                 n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n",
    "                                 flip_y=0.0, random_state=30 + k)\n",
    "    yk[:] = 0\n",
    "    X0_parts.append((Xk, yk))\n",
    "X0 = np.vstack([p[0] for p in X0_parts]); y0 = np.hstack([p[1] for p in X0_parts])\n",
    "\n",
    "\n",
    "sizes1 = [n_per//3, n_per//3, n_per - 2*(n_per//3)]\n",
    "X1_parts = []\n",
    "for k, ns in enumerate(sizes1):\n",
    "    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n",
    "                                 n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n",
    "                                 flip_y=0.0, random_state=40 + k)\n",
    "    yk[:] = 1\n",
    "    X1_parts.append((Xk, yk))\n",
    "X1 = np.vstack([p[0] for p in X1_parts]); y1 = np.hstack([p[1] for p in X1_parts])\n",
    "\n",
    "\n",
    "sizes2 = [n_per//4, n_per//4, n_per//4, n_per - 3*(n_per//4)]\n",
    "X2_parts = []\n",
    "for k, ns in enumerate(sizes2):\n",
    "    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n",
    "                                 n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n",
    "                                 flip_y=0.0, random_state=50 + k)\n",
    "    yk[:] = 2\n",
    "    X2_parts.append((Xk, yk))\n",
    "X2 = np.vstack([p[0] for p in X2_parts]); y2 = np.hstack([p[1] for p in X2_parts])\n",
    "\n",
    "X = np.vstack([X0, X1, X2]).astype(float)\n",
    "y = np.hstack([y0, y1, y2]).astype(int)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "C = 3\n",
    "ytr_oh = np.eye(C)[ytr]\n",
    "yte_oh = np.eye(C)[yte]\n",
    "\n",
    "\n",
    "in_features = 4\n",
    "h = 64\n",
    "out_features = 3\n",
    "\n",
    "W1 = rng.normal(0, 1, (h, in_features)) / np.sqrt(in_features)\n",
    "b1 = np.zeros((h,))\n",
    "W2 = rng.normal(0, 1, (out_features, h)) / np.sqrt(h)\n",
    "b2 = np.zeros((out_features,))\n",
    "\n",
    "lr = 5e-2\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "eps = 1e-12\n",
    "\n",
    "N = Xtr.shape[0]\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    idx = rng.permutation(N)\n",
    "    Xs = Xtr[idx]; ys = ytr_oh[idx]\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        xb = Xs[i:i+batch_size]\n",
    "        yb = ys[i:i+batch_size]\n",
    "\n",
    "\n",
    "        z1 = xb @ W1.T + b1\n",
    "        h1 = np.tanh(z1)\n",
    "\n",
    "        U = h1 @ W2.T + b2\n",
    "        Umax = U.max(axis=1, keepdims=True)\n",
    "        ex = np.exp(U - Umax)\n",
    "        yhat = ex / ex.sum(axis=1, keepdims=True)\n",
    "\n",
    "        yclip = np.clip(yhat, eps, 1-eps)\n",
    "        loss = -np.mean(np.sum(yb * np.log(yclip), axis=1))\n",
    "\n",
    "        B = yb.shape[0]\n",
    "        dL_dU = (yhat - yb) / B \n",
    "\n",
    "        dL_dW2 = dL_dU.T @ h1\n",
    "        dL_db2 = dL_dU.sum(axis=0)\n",
    "\n",
    "        dL_dh1 = dL_dU @ W2\n",
    "        dL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n",
    "\n",
    "        dL_dW1 = dL_dz1.T @ xb\n",
    "        dL_db1 = dL_dz1.sum(axis=0)\n",
    "\n",
    "        W2 -= lr * dL_dW2\n",
    "        b2 -= lr * dL_db2\n",
    "        W1 -= lr * dL_dW1\n",
    "        b1 -= lr * dL_db1\n",
    "\n",
    "    if ep % 50 == 0 or ep == 1:\n",
    "        print(f\"epoch {ep:3d} | loss {loss:.4f}\")\n",
    "\n",
    "z1 = Xte @ W1.T + b1\n",
    "h1 = np.tanh(z1)\n",
    "U = h1 @ W2.T + b2\n",
    "Umax = U.max(axis=1, keepdims=True)\n",
    "ex = np.exp(U - Umax)\n",
    "yhat = ex / ex.sum(axis=1, keepdims=True)\n",
    "ypred = np.argmax(yhat, axis=1)\n",
    "acc = (ypred == yte).mean()\n",
    "print(f\"\\nTest accuracy (3 classes): {acc:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055461a5",
   "metadata": {},
   "source": [
    "### Excercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a73d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1 | loss 0.9812\n",
      "epoch  50 | loss 0.7336\n",
      "epoch 100 | loss 0.6125\n",
      "epoch 150 | loss 0.5455\n",
      "epoch 200 | loss 0.3711\n",
      "epoch 250 | loss 0.4223\n",
      "epoch 300 | loss 0.4229\n",
      "epoch 350 | loss 0.4302\n",
      "\n",
      "Test accuracy (deeper, 2 hidden): 0.700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rng = np.random.default_rng(321)\n",
    "\n",
    "\n",
    "n_total = 1500\n",
    "n_per = n_total // 3\n",
    "\n",
    "\n",
    "sizes0 = [n_per//2, n_per - n_per//2]\n",
    "X0_parts = []\n",
    "for k, ns in enumerate(sizes0):\n",
    "    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n",
    "                                 n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n",
    "                                 flip_y=0.0, random_state=70 + k)\n",
    "    yk[:] = 0\n",
    "    X0_parts.append((Xk, yk))\n",
    "X0 = np.vstack([p[0] for p in X0_parts]); y0 = np.hstack([p[1] for p in X0_parts])\n",
    "\n",
    "\n",
    "sizes1 = [n_per//3, n_per//3, n_per - 2*(n_per//3)]\n",
    "X1_parts = []\n",
    "for k, ns in enumerate(sizes1):\n",
    "    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n",
    "                                 n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n",
    "                                 flip_y=0.0, random_state=80 + k)\n",
    "    yk[:] = 1\n",
    "    X1_parts.append((Xk, yk))\n",
    "X1 = np.vstack([p[0] for p in X1_parts]); y1 = np.hstack([p[1] for p in X1_parts])\n",
    "\n",
    "\n",
    "sizes2 = [n_per//4, n_per//4, n_per//4, n_per - 3*(n_per//4)]\n",
    "X2_parts = []\n",
    "for k, ns in enumerate(sizes2):\n",
    "    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n",
    "                                 n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n",
    "                                 flip_y=0.0, random_state=90 + k)\n",
    "    yk[:] = 2\n",
    "    X2_parts.append((Xk, yk))\n",
    "X2 = np.vstack([p[0] for p in X2_parts]); y2 = np.hstack([p[1] for p in X2_parts])\n",
    "\n",
    "X = np.vstack([X0, X1, X2]).astype(float)\n",
    "y = np.hstack([y0, y1, y2]).astype(int)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "C = 3\n",
    "ytr_oh = np.eye(C)[ytr]\n",
    "\n",
    "in_features = 4\n",
    "h1_size = 64\n",
    "h2_size = 32\n",
    "out_features = 3\n",
    "\n",
    "W1 = rng.normal(0, 1, (h1_size, in_features)) / np.sqrt(in_features)\n",
    "b1 = np.zeros((h1_size,))\n",
    "\n",
    "W2 = rng.normal(0, 1, (h2_size, h1_size)) / np.sqrt(h1_size)\n",
    "b2 = np.zeros((h2_size,))\n",
    "\n",
    "W3 = rng.normal(0, 1, (out_features, h2_size)) / np.sqrt(h2_size)\n",
    "b3 = np.zeros((out_features,))\n",
    "\n",
    "lr = 3e-2\n",
    "epochs = 350\n",
    "batch_size = 64\n",
    "eps = 1e-12\n",
    "\n",
    "N = Xtr.shape[0]\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    idx = rng.permutation(N)\n",
    "    Xs = Xtr[idx]; ys = ytr_oh[idx]\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        xb = Xs[i:i+batch_size]\n",
    "        yb = ys[i:i+batch_size]\n",
    "\n",
    "\n",
    "        z1 = xb @ W1.T + b1\n",
    "        h1 = np.tanh(z1)\n",
    "\n",
    "        z2 = h1 @ W2.T + b2\n",
    "        h2 = np.tanh(z2)\n",
    "\n",
    "        U = h2 @ W3.T + b3 \n",
    "        Umax = U.max(axis=1, keepdims=True)\n",
    "        ex = np.exp(U - Umax)\n",
    "        yhat = ex / ex.sum(axis=1, keepdims=True)\n",
    "\n",
    "       \n",
    "        yclip = np.clip(yhat, eps, 1-eps)\n",
    "        loss = -np.mean(np.sum(yb * np.log(yclip), axis=1))\n",
    "\n",
    "        \n",
    "        B = yb.shape[0]\n",
    "        dL_dU = (yhat - yb) / B  \n",
    "\n",
    "        dL_dW3 = dL_dU.T @ h2\n",
    "        dL_db3 = dL_dU.sum(axis=0)\n",
    "\n",
    "        dL_dh2 = dL_dU @ W3\n",
    "        dL_dz2 = dL_dh2 * (1 - np.tanh(z2)**2)\n",
    "\n",
    "        dL_dW2 = dL_dz2.T @ h1\n",
    "        dL_db2 = dL_dz2.sum(axis=0)\n",
    "\n",
    "        dL_dh1 = dL_dz2 @ W2\n",
    "        dL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n",
    "\n",
    "        dL_dW1 = dL_dz1.T @ xb\n",
    "        dL_db1 = dL_dz1.sum(axis=0)\n",
    "\n",
    "        W3 -= lr * dL_dW3\n",
    "        b3 -= lr * dL_db3\n",
    "        W2 -= lr * dL_dW2\n",
    "        b2 -= lr * dL_db2\n",
    "        W1 -= lr * dL_dW1\n",
    "        b1 -= lr * dL_db1\n",
    "\n",
    "    if ep % 50 == 0 or ep == 1:\n",
    "        print(f\"epoch {ep:3d} | loss {loss:.4f}\")\n",
    "\n",
    "\n",
    "z1 = Xte @ W1.T + b1\n",
    "h1 = np.tanh(z1)\n",
    "z2 = h1 @ W2.T + b2\n",
    "h2 = np.tanh(z2)\n",
    "U = h2 @ W3.T + b3\n",
    "Umax = U.max(axis=1, keepdims=True)\n",
    "ex = np.exp(U - Umax)\n",
    "yhat = ex / ex.sum(axis=1, keepdims=True)\n",
    "ypred = np.argmax(yhat, axis=1)\n",
    "acc = (ypred == yte).mean()\n",
    "print(f\"\\nTest accuracy (deeper, 2 hidden): {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
