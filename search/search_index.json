{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"data/","title":"Atividade: 1. Data","text":""},{"location":"data/#exercicio-1-exploring-class-separability-in-2d","title":"Exerc\u00edcio 1 \u2014 Exploring Class Separability in 2D","text":""},{"location":"data/#a-brief-description-of-my-approach","title":"A brief description of my approach","text":"<p>Para gerar os dados, utilizo distribui\u00e7\u00f5es normais multivariadas com a fun\u00e7\u00e3o random.multivariate_normal do numpy, cada um com m\u00e9dia e covari\u00e2ncia diferentes. Cada ponto recebe um r\u00f3tulo de classe correspondente ao cluster de origem.</p>"},{"location":"data/#generate-the-data","title":"Generate  the data:","text":"<pre><code>data = {'x': [], 'y':[], 'class':[], 'color': []}\n\n\nmean_std_dev = [([2,3], np.diag([0.8, 2.5])) ,\n([5,6], np.diag([1.2, 1.9])),\n([8,1], np.diag([0.9, 0.9])),\n([15,4], np.diag([0.5, 2.0]))]\nmean = [2, 3]\ncov = np.diag([0.8, 2.5])\nfor i in range(0, 4):\n\n    mean = mean_std_dev[i][0]\n    cov = mean_std_dev[i][1]\n\n    x, y = np.random.multivariate_normal(mean, cov, 100).T\n    classe = []\n    for c in range(0, len(x)):\n        data['x'].append(x[c])\n        data['y'].append(y[c])\n        data['class'].append(f\"class_{i}\")\n        if i == 0:\n            data['color'].append((1.0, 0.0, 0.0))\n        if i == 1:\n            data['color'].append((1.0,0.0,1.0,))\n        if i == 2:\n            data['color'].append((1.0, 1.0, 0.0))\n        if i == 3:\n            data['color'].append((1.0, 0.5, 0.5))\n</code></pre>"},{"location":"data/#ploting-the-data","title":"Ploting the Data","text":""},{"location":"data/#analyze-and-draw-boundaries","title":"Analyze and Draw Boundaries","text":"<p>a .No gr\u00e1fico de dispers\u00e3o, observa-se que as duas classes mais \u00e0 direita (rosa claro) est\u00e3o bem separadas das demais, sem sobreposi\u00e7\u00e3o aparente. J\u00e1 as duas classes mais \u00e0 esquerda (vermelha e roxa) apresentam consider\u00e1vel sobreposi\u00e7\u00e3o em suas regi\u00f5es, tornando dif\u00edcil distingui-las linearmente. A classe amarela encontra-se abaixo, relativamente bem delimitada, embora mais pr\u00f3xima das classes vermelha e roxa.</p> <p>b. Observando o plot, seriam necess\u00e1rias tr\u00eas fronteiras lineares para separar corretamente todas as classes. Com apenas uma, seria poss\u00edvel dividir em metades (esquerda e direita), mas cada lado ainda conteria duas classes. Com duas fronteiras, ainda restaria um lado com duas classes agrupadas. Assim, somente com uma terceira linha seria poss\u00edvel separar essas duas classes restantes.</p> <p>c.</p>"},{"location":"data/#_1","title":"1.Data","text":""},{"location":"data/#exercicio-2-non-linearity-in-higher-dimensions","title":"Exerc\u00edcio 2 \u2014 Non-Linearity in Higher Dimensions","text":""},{"location":"data/#a-brief-description-of-my-approach_1","title":"A brief description of my approach","text":"<p>Para geras os dados em cinco dimens\u00f5es foi utilizados distribui\u00e7\u00f5es normais multivariadas, cada um com atributos diferentes. Cada amostra recebeu um rotulo de classe (A ou B), dependendo da origem dela.</p>"},{"location":"data/#generate-the-data_1","title":"Generate  the data:","text":"<pre><code>import numpy as np\nimport pandas as pd\n\nrng = np.random.default_rng(42)\ndata = {\n    'x1': [], 'x2': [], 'x3': [], 'x4': [], 'x5': [],\n    'class': [], 'color': []\n}\nparams = [\n    (\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n        np.array([\n            [1.0, 0.8, 0.1, 0.0, 0.0],\n            [0.8, 1.0, 0.3, 0.0, 0.0],\n            [0.1, 0.3, 1.0, 0.5, 0.0],\n            [0.0, 0.0, 0.5, 1.0, 0.2],\n            [0.0, 0.0, 0.0, 0.2, 1.0],\n        ]),\n        \"class_A\",\n        (1.0, 0.0, 0.0), \n    ),\n    (\n        np.array([1.5, 1.5, 1.5, 1.5, 1.5]),\n        np.array([\n            [1.5, -0.7, 0.2, 0.0, 0.0],\n            [-0.7, 1.5, 0.4, 0.0, 0.0],\n            [0.2, 0.4, 1.5, 0.6, 0.0],\n            [0.0, 0.0, 0.6, 1.5, 0.3],\n            [0.0, 0.0, 0.0, 0.3, 1.5],\n        ]),\n        \"class_B\",\n        (0.0, 0.0, 1.0), \n    ),\n]\n\nn_per_class = 500\n\nfor mean, cov, label, color in params:\n    samples = rng.multivariate_normal(mean, cov, size=n_per_class)\n    for s in samples:\n        data['x1'].append(s[0])\n        data['x2'].append(s[1])\n        data['x3'].append(s[2])\n        data['x4'].append(s[3])\n        data['x5'].append(s[4])\n        data['class'].append(label)\n        data['color'].append(color)\n\n\ndf = pd.DataFrame(data)\n</code></pre>"},{"location":"data/#visualize-the-data","title":"Visualize the data","text":""},{"location":"data/#analyze-the-plot","title":"Analyze the plot","text":"<p>a. Observando o gr\u00e1fico, percebe-se que os pontos vermelhos tendem a se concentrar mais \u00e0 esquerda, enquanto os azuis ficam predominantemente \u00e0 direita. No entanto, h\u00e1 uma sobreposi\u00e7\u00e3o consider\u00e1vel: v\u00e1rios pontos vermelhos aparecem em regi\u00f5es azuis e vice-versa. Isso significa que uma simples linha no meio do gr\u00e1fico n\u00e3o conseguiria separar as classes com boa acur\u00e1cia, j\u00e1 que h\u00e1 mistura entre elas.</p> <p>b. Esse tipo de dado imp\u00f5e um desafio, pois um modelo linear teria dificuldade em capturar esses casos de sobreposi\u00e7\u00e3o. Seriam necess\u00e1rias m\u00faltiplas fronteiras lineares para tentar contornar o problema, o que aumenta o risco de overfitting. Por isso, uma rede neural com m\u00faltiplas camadas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares se mostra mais adequada, j\u00e1 que consegue modelar fronteiras de decis\u00e3o complexas e lidar melhor com regi\u00f5es de interse\u00e7\u00e3o entre as classes.</p>"},{"location":"data/#exercicio-3-preparing-real-world-data-for-a-neural-network","title":"Exerc\u00edcio 3 \u2014 Preparing Real-World Data for a Neural Network","text":""},{"location":"data/#a-brief-description-of-my-approach_2","title":"A brief description of my approach","text":"<p>Carrego o dataset e realizo o tratamento dos valores faltantes, usando a mediana para vari\u00e1veis num\u00e9ricas, a moda para vari\u00e1veis bin\u00e1rias e preenchendo as categ\u00f3ricas com \"Unknown\".</p> <p>Em seguida, aplico one-hot encoding nas vari\u00e1veis categ\u00f3ricas e normalizo os atributos num\u00e9ricos para a faixa [-1, 1].</p>"},{"location":"data/#describe-the-data","title":"Describe the Data","text":"<ul> <li> <p>O objetivo do dataset \"Spaceship Titanic\" \u00e9 prever se os passageiros foram transportados para outra dimens\u00e3o. A coluna \"Transported\" representa esse resultado: valor 1 indica que o passageiro foi transportado e valor 0 indica que n\u00e3o foi.</p> </li> <li> <p>Features num\u00e9ricas: Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck.</p> </li> <li> <p>Features categ\u00f3ricas: PassengerId, HomePlanet, CryoSleep, Cabin, Destination, VIP, Name.</p> </li> <li> <p>Valores faltantes por coluna:</p> </li> </ul> Coluna N\u00ba de faltantes CryoSleep 217 ShoppingMall 208 VIP 203 HomePlanet 201 Name 200 Cabin 199 VRDeck 188 FoodCourt 183 Spa 183 Destination 182 RoomService 181 Age 179"},{"location":"data/#preprocess-the-data","title":"Preprocess the Data","text":"<pre><code>def report_missing(df):\n    miss = df.isna().sum()\n    miss = miss[miss &gt; 0].sort_values(ascending=False)\n    print(\"\\nFaltantes por coluna:\\n\" + (miss.to_string() if len(miss) else \"Nenhum\"))\n    return miss\n\ndef one_hot_manual(series, prefix=None):\n    s = series.astype(\"category\")\n    cats = list(s.cat.categories)\n    out = pd.DataFrame(index=series.index)\n    for c in cats:\n        col = f\"{prefix or series.name}__{c}\"\n        out[col] = (s == c).astype(int)\n    return out\n\ndef minmax_pm1_df(df, cols):\n    mn = df[cols].min(axis=0)\n    mx = df[cols].max(axis=0)\n    denom = (mx - mn).replace(0, 1.0)\n    scaled = 2 * ((df[cols] - mn) / denom) - 1\n    return scaled, mn, mx\n\n\ndef prepare_spaceship_norm(path=CSV_PATH):\n    df = pd.read_csv(path)\n    print(\"Carregado:\", path, \"shape:\", df.shape)\n\n    num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n    bin_cols = [\"CryoSleep\", \"VIP\"]           \n    cat_cols = [\"HomePlanet\", \"Destination\"]   \n    target = \"Transported\"\n    for c in num_cols:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    y = df[target].astype(int)\n\n    report_missing(df)\n    for c in num_cols:\n        df[c] = df[c].fillna(df[c].median())\n    for c in bin_cols:\n        m = df[c].mode(dropna=True)\n        fill = m.iloc[0] if len(m) else False\n        df[c] = df[c].fillna(fill).map({True: 1, False: 0}).astype(int)\n    for c in cat_cols:\n        df[c] = df[c].fillna(\"Unknown\").astype(str)\n\n    oh_parts = [one_hot_manual(df[c], prefix=c) for c in cat_cols]\n    X_cat = pd.concat(oh_parts, axis=1)\n    X_num_scaled, mn, mx = minmax_pm1_df(df, num_cols)\n\n    X = pd.concat([X_num_scaled[num_cols], df[bin_cols].astype(int), X_cat], axis=1)\n    print(\"Features finais:\", X.shape)\n\n    plt.figure(figsize=(6,4)); df[\"Age\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"Age \u2014 ANTES (valor bruto)\"); plt.xlabel(\"Age\"); plt.tight_layout()\n\n    plt.figure(figsize=(6,4)); X_num_scaled[\"Age\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"Age \u2014 DEPOIS (normalizado [-1,1])\"); plt.xlabel(\"Age norm.\"); plt.tight_layout()\n\n    plt.figure(figsize=(6,4)); df[\"FoodCourt\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"FoodCourt \u2014 ANTES (valor bruto)\"); plt.xlabel(\"Gasto\"); plt.tight_layout()\n\n    plt.figure(figsize=(6,4)); X_num_scaled[\"FoodCourt\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"FoodCourt \u2014 DEPOIS (normalizado [-1,1])\"); plt.xlabel(\"Gasto norm.\"); plt.tight_layout()\n\n    plt.show()\n    return X, y, {\"min\": mn, \"max\": mx}\n</code></pre>"},{"location":"data/#visualize-the-results","title":"Visualize the Results","text":""},{"location":"data/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Descri\u00e7\u00e3o do Spaceship Titanic e do alvo <code>Transported</code>: https://www.kaggle.com/competitions/spaceship-titanic/data.  </li> </ul>"},{"location":"perceptron/","title":"Atividade: 2. Perceptron","text":""},{"location":"perceptron/#exercicio-1","title":"Exerc\u00edcio 1","text":""},{"location":"perceptron/#a-brief-description-of-my-approach","title":"A brief description of my approach","text":"<p>Para gerar os dados utilizo a fun\u00e7\u00e3o <code>multivariate_normal</code>, que permite passar o vetor de m\u00e9dias e a matriz de covari\u00e2ncia para aplicar a distribui\u00e7\u00e3o <code>Gaussiana</code> multivariada. Em seguida, aplico um vstack para formar o vetor X, empilhando os arrays verticalmente, enquanto para o vetor y utilizo um hstack, que os empilha horizontalmente.</p> <p>Na implementa\u00e7\u00e3o do Perceptron, crio um loop externo para limitar o n\u00famero m\u00e1ximo de \u00e9pocas, caso n\u00e3o ocorra a converg\u00eancia. Dentro desse loop, antes de entrar no interno, inicializo as vari\u00e1veis updates e correct com zero. No loop interno, verifico se ocorre algum erro; em caso positivo, recalculo os pesos e o vi\u00e9s. Caso n\u00e3o ocorra nenhum erro durante toda a \u00e9poca, significa que houve converg\u00eancia, e o loop \u00e9 interrompido.</p>"},{"location":"perceptron/#generate-the-data","title":"Generate  the data:","text":"<pre><code>mean_class0 = np.array([1.5, 1.5])\nmean_class1 = np.array([5.0, 5.0])\ncov   = np.array([[0.5, 0.0],[0.0, 0.5]])\nX_class0 = rng.multivariate_normal(mean_class0, cov, size=samples_per_class)\nX_class1 = rng.multivariate_normal(mean_class1, cov, size=samples_per_class)\nX  = np.vstack([X_class0, X_class1])\ny  = np.hstack([np.zeros(samples_per_class, dtype=int), np.ones(samples_per_class,  dtype=int)])\n</code></pre>"},{"location":"perceptron/#ploting-the-data","title":"Ploting the Data","text":""},{"location":"perceptron/#perceptron-implementation-task","title":"Perceptron Implementation Task:","text":"<p><pre><code>r=0.01 \nmax_epochs=100 \n\n\nrng = np.random.default_rng(seed)\nn, d = X.shape\nw = np.zeros(d)\nb = 0.0\nacc_hist = []\n\nfor epoch in range(1, max_epochs+1):\n    idx = np.arange(n)\n\n    rng.shuffle(idx)\n    updates = 0\n    correct = 0\n\n    for i in idx:\n        z = np.dot(w, X[i]) + b\n        y_hat = 1 if z &gt;= 0 else 0\n        e = y[i] - y_hat \n        if e != 0:\n\n            w = w + lr * e * X[i]\n            b = b + lr * e\n            updates += 1\n        else:\n            correct += 1\n\n        acc = correct / n\n        acc_hist.append(acc)\n\n        if updates == 0:\n            break\n</code></pre> </p>"},{"location":"perceptron/#reporting-results","title":"Reporting results","text":"<p>Pesos finais w: [0.04894919 0.06400311] Vi\u00e9s final b: -0.3200000000000001 accuracia finalaa: 99.60%</p> <p>Para entender o motivo da converg\u00eancia r\u00e1pida, \u00e9 necess\u00e1rio explicar que o Perceptron funciona com uma regra de atualiza\u00e7\u00e3o: sempre que um ponto \u00e9 classificado incorretamente, os pesos s\u00e3o ajustados na dire\u00e7\u00e3o correta para reduzir o erro. Neste caso, como h\u00e1 uma separabilidade linear clara entre as classes, o algoritmo atinge a converg\u00eancia rapidamente, pois n\u00e3o h\u00e1 necessidade de muitas atualiza\u00e7\u00f5es para encontrar uma fronteira de decis\u00e3o adequada.</p> <p></p>"},{"location":"perceptron/#exercicio-2","title":"Exerc\u00edcio 2","text":""},{"location":"perceptron/#a-brief-description-of-my-approach_1","title":"A brief description of my approach","text":"<p>Neste exerc\u00edcio os dados s\u00e3o gerados da mesma forma que no primeiro, mas o Perceptron \u00e9 treinado em 5 execu\u00e7\u00f5es com inicializa\u00e7\u00f5es diferentes. Para cada execu\u00e7\u00e3o registro a acur\u00e1cia por \u00e9poca e, ao final, calculo a m\u00e9dia e o desvio-padr\u00e3o das acur\u00e1cias, alinhando os hist\u00f3ricos quando necess\u00e1rio. Isso permite avaliar o desempenho m\u00e9dio do modelo em dados com sobreposi\u00e7\u00e3o e reduzir a influ\u00eancia da inicializa\u00e7\u00e3o aleat\u00f3ria.</p>"},{"location":"perceptron/#data-generation-task","title":"Data Generation Task:","text":"<pre><code>mean_class0 = np.array([3.0, 3.0])\nmean_class1 = np.array([4.0, 4.0])\ncov   = np.array([[1.5, 0.0],[0.0, 1.5]])\nX_class0 = rng.multivariate_normal(mean_class0, cov, size=samples_per_class)\nX_class1 = rng.multivariate_normal(mean_class1, cov, size=samples_per_class)\nX  = np.vstack([X_class0, X_class1])\ny  = np.hstack([np.zeros(samples_per_class, dtype=int), np.ones(samples_per_class,  dtype=int)])\n</code></pre>"},{"location":"perceptron/#visualize-the-data","title":"Visualize the data","text":""},{"location":"perceptron/#perceptron-implementation-task_1","title":"Perceptron implementation task","text":"<pre><code>num_runs = 5\nall_hist = []\nfinal_accs = []\nfinal_ws = []\nfinal_bs = []\n\nfor run in range(num_runs):\n    local_rng = np.random.default_rng(100 + run)\n\n    w_r = local_rng.normal(0, 0.01, size=X.shape[1])\n    b_r = 0.0\n\n    acc_hist_r = []\n    idx = np.arange(y.shape[0])\n\n    for epoch in range(1, 101):  \n        local_rng.shuffle(idx)\n        updates = 0\n        correct = 0\n\n        for i in idx:\n            xi = X[i]\n            yi = y[i]\n            z  = float(np.dot(w_r, xi) + b_r)\n            yhat = 1 if z &gt;= 0 else 0\n            err  = int(yi - yhat)\n\n            if err != 0:\n                w_r = w_r + 0.01 * err * xi\n                b_r = b_r + 0.01 * err\n                updates += 1\n            else:\n                correct += 1\n\n        acc = correct / len(y)\n        acc_hist_r.append(acc)\n\n        if updates == 0:  \n            break\n\n\n    all_hist.append(acc_hist_r)\n    y_pred_r = ((X @ w_r + b_r) &gt;= 0).astype(int)\n    final_accs.append((y_pred_r == y).mean())\n    final_ws.append(w_r.copy())\n    final_bs.append(b_r)\n\n\nmax_len = max(len(h) for h in all_hist)\naligned = []\nfor h in all_hist:\n    if len(h) &lt; max_len:\n        h = h + [h[-1]] * (max_len - len(h))\n    aligned.append(h)\n\naligned = np.array(aligned)\nmean_acc = aligned.mean(axis=0)\nstd_acc  = aligned.std(axis=0)\n\n\nfor i in range(num_runs):\n    print(f\"Run {i+1}:\")\n    print(f\"  Final weights: {final_ws[i]}\")\n    print(f\"  Final bias: {final_bs[i]}\")\n    print(f\"  Final accuracy: {final_accs[i]*100:.2f}%\")\n    print()\n\nprint(\"Summary across runs:\")\nprint(\"  Final accuracies:\", [f\"{a*100:.2f}%\" for a in final_accs])\nprint(f\"  Mean final accuracy: {np.mean(final_accs)*100:.2f}%\")\nprint(f\"  Std final accuracy: {np.std(final_accs)*100:.2f}%\")\n</code></pre>"},{"location":"perceptron/#reporting-results_1","title":"Reporting results","text":"<p>Run 1:   Final weights: [-0.02059995  0.1328929 ]   Final bias: -0.43000000000000016   Final accuracy: 63.00%</p> <p>Run 2:   Final weights: [0.05533817 0.05429294]   Final bias: -0.42000000000000015   Final accuracy: 69.65%</p> <p>Run 3:   Final weights: [0.05812441 0.09586418]   Final bias: -0.41000000000000014   Final accuracy: 64.50%</p> <p>Run 4:   Final weights: [0.00919726 0.05628983]   Final bias: -0.43000000000000016   Final accuracy: 50.45%</p> <p>Run 5:   Final weights: [0.13061734 0.03119603]   Final bias: -0.41000000000000014   Final accuracy: 60.85%</p> <p>Summary across runs:   Final accuracies: ['63.00%', '69.65%', '64.50%', '50.45%', '60.85%']   Mean final accuracy: 61.69%   Std final accuracy: 6.32%</p> <p></p> <p>Esses dados, diferentemente do primeiro caso, n\u00e3o apresentam uma separabilidade linear clara, como pode ser observado na imagem. Isso torna dif\u00edcil para o Perceptron alcan\u00e7ar a converg\u00eancia, pois sempre haver\u00e1 pontos que n\u00e3o podem ser separados corretamente por uma \u00fanica reta. Assim, o algoritmo permanece ajustando os pesos e o vi\u00e9s a cada \u00e9poca, sem conseguir estabilizar em uma solu\u00e7\u00e3o perfeita.</p>"}]}