{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bem-vindo","text":""},{"location":"#disclaimer","title":"Disclaimer","text":"<p>As atividades apresentadas neste site foram desenvolvidas com assist\u00eancia de Intelig\u00eancia Artificial (ChatGPT), utilizada com o objetivo de:</p> <ul> <li>Aprimorar a reda\u00e7\u00e3o de textos nas documenta\u00e7\u00f5es, corrigindo ortografia e estilo;  </li> <li>Gerar e personalizar gr\u00e1ficos para melhor visualiza\u00e7\u00e3o dos resultados;  </li> <li>Otimizar o layout e a organiza\u00e7\u00e3o visual das p\u00e1ginas.  </li> </ul>"},{"location":"#repositorios-dos-trabalhos-em-grupo","title":"Reposit\u00f3rios dos Trabalhos em Grupo","text":""},{"location":"#projeto-1-classification","title":"Projeto 1 \u2014 Classification","text":"<p>P\u00e1gina do projeto: \ud83d\udd17 https://github.com/lucasouzamil/mlp-classification/deployments/github-pages</p>"},{"location":"#projeto-2-regression-ann","title":"Projeto 2 \u2014 Regression (ANN)","text":"<p>P\u00e1gina do projeto: \ud83d\udd17 https://lucasouzamil.github.io/ann-regression</p>"},{"location":"MLP/","title":"Atividade: 3. MLP","text":""},{"location":"MLP/#exercicio-1","title":"Exerc\u00edcio 1","text":""},{"location":"MLP/#a-brief-description-of-my-approach","title":"A brief description of my approach","text":"<p>Implemento, em c\u00f3digo direto e sem fun\u00e7\u00f5es, o fluxo completo forward \u2192 loss \u2192 backward \u2192 update de um MLP com tanh na camada escondida e na sa\u00edda, usando MSE para uma \u00fanica amostra. Imprimo todas as grandezas intermedi\u00e1rias (pr\u00e9-ativa\u00e7\u00f5es, ativa\u00e7\u00f5es, gradientes de pesos e vieses) e fa\u00e7o a atualiza\u00e7\u00e3o com \u03b7 = 0,1, para validar numericamente a cadeia de derivadas que ser\u00e1 reutilizada nos demais exerc\u00edcios.</p>"},{"location":"MLP/#code","title":"Code:","text":"<pre><code>import numpy as np\n\n\nx = np.array([0.5, -0.2], dtype=float)\ny = 1.0\n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]], dtype=float)\nb1 = np.array([ 0.1, -0.2], dtype=float)\n\nW2 = np.array([0.5, -0.3], dtype=float)  \nb2 = 0.2\neta_update = 0.1\n\n\nz1 = W1 @ x + b1\nh1 = np.tanh(z1)\nu2 = W2 @ h1 + b2\nyhat = np.tanh(u2)\n\n\nL = (y - yhat)**2\n\nprint(\"Forward pass:\")\nprint(\"z1 =\", z1)\nprint(\"h1 = tanh(z1) =\", h1)\nprint(\"u2 =\", u2)\nprint(\"yhat = tanh(u2) =\", yhat)\nprint(\"Loss L =\", L)\n\n\ndL_dyhat = 2*(yhat - y)               \nd_tanh_u2 = 1 - np.tanh(u2)**2\ndL_du2 = dL_dyhat * d_tanh_u2\n\n\ndL_dW2 = dL_du2 * h1                 \ndL_db2 = dL_du2\n\n\ndL_dh1 = dL_du2 * W2\nd_tanh_z1 = 1 - np.tanh(z1)**2\ndL_dz1 = dL_dh1 * d_tanh_z1            \ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\n\nprint(\"\\nBackward pass (gradients):\")\nprint(\"dL/dyhat =\", dL_dyhat)\nprint(\"dL/du2   =\", dL_du2)\nprint(\"dL/dW2   =\", dL_dW2)\nprint(\"dL/db2   =\", dL_db2)\nprint(\"dL/dh1   =\", dL_dh1)\nprint(\"dL/dz1   =\", dL_dz1)\nprint(\"dL/dW1   =\\n\", dL_dW1)\nprint(\"dL/db1   =\", dL_db1)\n\n\nW2_new = W2 - eta_update * dL_dW2\nb2_new = b2 - eta_update * dL_db2\nW1_new = W1 - eta_update * dL_dW1\nb1_new = b1 - eta_update * dL_db1\n\nprint(\"\\nUpdated parameters (eta = 0.1):\")\nprint(\"W2_new =\", W2_new)\nprint(\"b2_new =\", b2_new)\nprint(\"W1_new =\\n\", W1_new)\nprint(\"b1_new =\", b1_new)\n</code></pre>"},{"location":"MLP/#result","title":"Result:","text":"<pre><code>```bash\nForward pass:\nz1 = [ 0.27 -0.18]\nh1 = tanh(z1) = [ 0.26362484 -0.17808087]\nu2 = 0.38523667817130075\nyhat = tanh(u2) = 0.36724656264510797\nLoss L = 0.4003769124844312\n\nBackward pass (gradients):\ndL/dyhat = -1.265506874709784\ndL/du2   = -1.0948279147135995\ndL/dW2   = [-0.28862383  0.19496791]\ndL/db2   = -1.0948279147135995\ndL/dh1   = [-0.54741396  0.32844837]\ndL/dz1   = [-0.50936975  0.31803236]\ndL/dW1   =\n[[-0.25468488  0.10187395]\n[ 0.15901618 -0.06360647]]\ndL/db1   = [-0.50936975  0.31803236]\n\nUpdated parameters (eta = 0.1):\nW2_new = [ 0.52886238 -0.31949679]\nb2_new = 0.30948279147136\nW1_new =\n[[ 0.32546849 -0.1101874 ]\n[ 0.18409838  0.40636065]]\nb1_new = [ 0.15093698 -0.23180324]\n\n```\n</code></pre>"},{"location":"MLP/#exercicio-2","title":"Exerc\u00edcio 2","text":""},{"location":"MLP/#a-brief-description-of-my-approach_1","title":"A brief description of my approach","text":"<p>Gero um conjunto bin\u00e1rio com make_classification, em que a classe 0 tem 1 cluster e a classe 1 tem 2 clusters. Treino um MLP 2\u201332\u20131 escrito inline tanh na escondida, sigmoid na sa\u00edda e Binary Cross-Entropy como perda. O treinamento \u00e9 via mini-batch gradient descent, com retropropaga\u00e7\u00e3o manual dos gradientes e avalia\u00e7\u00e3o por acur\u00e1cia no conjunto de teste.</p>"},{"location":"MLP/#code_1","title":"Code:","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nrng = np.random.default_rng(42)\n\nn_total = 1000\nX0, y0 = make_classification(\n    n_samples=n_total//2, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=1, n_classes=2, class_sep=1.3, flip_y=0.0, random_state=7\n)\ny0 = (y0==0).astype(int) \n\nX1a, y1a = make_classification(\n    n_samples=n_total//4, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=1, n_classes=2, class_sep=1.3, flip_y=0.0, random_state=8\n)\nX1b, y1b = make_classification(\n    n_samples=n_total - (n_total//2 + n_total//4), n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=1, n_classes=2, class_sep=1.3, flip_y=0.0, random_state=9\n)\ny1a[:] = 1\ny1b[:] = 1\n\nX = np.vstack([X0, X1a, X1b]).astype(float)\ny = np.hstack([y0, y1a, y1b]).astype(int)\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\nin_features = 2\nh = 32\n\nW1 = rng.normal(0, 1, (h, in_features)) / np.sqrt(in_features)\nb1 = np.zeros((h,))\nW2 = rng.normal(0, 1, (1, h)) / np.sqrt(h)\nb2 = np.zeros((1,))\n\nlr = 5e-2\nepochs = 300\nbatch_size = 64\n\n\nytr_ = ytr.reshape(-1, 1).astype(float)\nN = Xtr.shape[0]\neps = 1e-12\n\nfor ep in range(1, epochs+1):\n    idx = rng.permutation(N)\n    Xs = Xtr[idx]; ys = ytr_[idx]\n\n    for i in range(0, N, batch_size):\n        xb = Xs[i:i+batch_size]\n        yb = ys[i:i+batch_size]\n\n\n        z1 = xb @ W1.T + b1\n        h1 = np.tanh(z1)\n\n        u2 = h1 @ W2.T + b2 \n        yhat = 1.0 / (1.0 + np.exp(-u2))\n\n        yclip = np.clip(yhat, eps, 1-eps)\n        loss = -np.mean(yb*np.log(yclip) + (1-yb)*np.log(1-yclip))\n\n\n        B = yb.shape[0]\n        dL_dyhat = (yhat - yb) / (yclip*(1-yclip)) / B\n        dL_du2 = dL_dyhat * (yhat*(1-yhat))  \n\n        dL_dW2 = dL_du2.T @ h1\n        dL_db2 = dL_du2.sum(axis=0)\n\n        dL_dh1 = dL_du2 @ W2\n        dL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n\n        dL_dW1 = dL_dz1.T @ xb\n        dL_db1 = dL_dz1.sum(axis=0)\n\n\n        W2 -= lr * dL_dW2\n        b2 -= lr * dL_db2\n        W1 -= lr * dL_dW1\n        b1 -= lr * dL_db1\n\n    if ep % 50 == 0 or ep == 1:\n        print(f\"epoch {ep:3d} | loss {loss:.4f}\")\n\n\nz1 = Xte @ W1.T + b1\nh1 = np.tanh(z1)\nu2 = h1 @ W2.T + b2\nyhat = 1.0 / (1.0 + np.exp(-u2))\nypred = (yhat.ravel() &gt;= 0.5).astype(int)\nacc = (ypred == yte).mean()\nprint(f\"\\nTest accuracy (binary): {acc:.3f}\")\n</code></pre>"},{"location":"MLP/#result_1","title":"Result:","text":"<pre><code>```bash\nepoch   1 | loss 0.4612\nepoch  50 | loss 0.2585\nepoch 100 | loss 0.1581\nepoch 150 | loss 0.2104\nepoch 200 | loss 0.1864\nepoch 250 | loss 0.2356\nepoch 300 | loss 0.1400\n\nTest accuracy (binary): 0.890\n\n```\n</code></pre>"},{"location":"MLP/#exercicio-3","title":"Exerc\u00edcio 3","text":""},{"location":"MLP/#a-brief-description-of-my-approach_2","title":"A brief description of my approach","text":"<p>Formulo um problema multiclasse (3 classes, 4 features), construindo 2/3/4 clusters por classe pela uni\u00e3o de subconjuntos. Treino um MLP 4\u201364\u20133 tamb\u00e9m inline, trocando a cabe\u00e7a para softmax + cross-entropy. Para estabilidade num\u00e9rica, subtraio o m\u00e1ximo por amostra antes do exp e aplico clipping nas probabilidades; a l\u00f3gica de treino e retroprop permanece a mesma do Ex. 2, evidenciando reuso conceitual.</p>"},{"location":"MLP/#code_2","title":"Code:","text":"<pre><code>```python\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nrng = np.random.default_rng(123)\n\nn_total = 1500\nn_per = n_total // 3\n\n\nsizes0 = [n_per//2, n_per - n_per//2]\nX0_parts = []\nfor k, ns in enumerate(sizes0):\n    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n                                n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n                                flip_y=0.0, random_state=30 + k)\n    yk[:] = 0\n    X0_parts.append((Xk, yk))\nX0 = np.vstack([p[0] for p in X0_parts]); y0 = np.hstack([p[1] for p in X0_parts])\n\n\nsizes1 = [n_per//3, n_per//3, n_per - 2*(n_per//3)]\nX1_parts = []\nfor k, ns in enumerate(sizes1):\n    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n                                n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n                                flip_y=0.0, random_state=40 + k)\n    yk[:] = 1\n    X1_parts.append((Xk, yk))\nX1 = np.vstack([p[0] for p in X1_parts]); y1 = np.hstack([p[1] for p in X1_parts])\n\n\nsizes2 = [n_per//4, n_per//4, n_per//4, n_per - 3*(n_per//4)]\nX2_parts = []\nfor k, ns in enumerate(sizes2):\n    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n                                n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n                                flip_y=0.0, random_state=50 + k)\n    yk[:] = 2\n    X2_parts.append((Xk, yk))\nX2 = np.vstack([p[0] for p in X2_parts]); y2 = np.hstack([p[1] for p in X2_parts])\n\nX = np.vstack([X0, X1, X2]).astype(float)\ny = np.hstack([y0, y1, y2]).astype(int)\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nC = 3\nytr_oh = np.eye(C)[ytr]\nyte_oh = np.eye(C)[yte]\n\n\nin_features = 4\nh = 64\nout_features = 3\n\nW1 = rng.normal(0, 1, (h, in_features)) / np.sqrt(in_features)\nb1 = np.zeros((h,))\nW2 = rng.normal(0, 1, (out_features, h)) / np.sqrt(h)\nb2 = np.zeros((out_features,))\n\nlr = 5e-2\nepochs = 300\nbatch_size = 64\neps = 1e-12\n\nN = Xtr.shape[0]\n\nfor ep in range(1, epochs+1):\n    idx = rng.permutation(N)\n    Xs = Xtr[idx]; ys = ytr_oh[idx]\n\n    for i in range(0, N, batch_size):\n        xb = Xs[i:i+batch_size]\n        yb = ys[i:i+batch_size]\n\n\n        z1 = xb @ W1.T + b1\n        h1 = np.tanh(z1)\n\n        U = h1 @ W2.T + b2\n        Umax = U.max(axis=1, keepdims=True)\n        ex = np.exp(U - Umax)\n        yhat = ex / ex.sum(axis=1, keepdims=True)\n\n        yclip = np.clip(yhat, eps, 1-eps)\n        loss = -np.mean(np.sum(yb * np.log(yclip), axis=1))\n\n        B = yb.shape[0]\n        dL_dU = (yhat - yb) / B \n\n        dL_dW2 = dL_dU.T @ h1\n        dL_db2 = dL_dU.sum(axis=0)\n\n        dL_dh1 = dL_dU @ W2\n        dL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n\n        dL_dW1 = dL_dz1.T @ xb\n        dL_db1 = dL_dz1.sum(axis=0)\n\n        W2 -= lr * dL_dW2\n        b2 -= lr * dL_db2\n        W1 -= lr * dL_dW1\n        b1 -= lr * dL_db1\n\n    if ep % 50 == 0 or ep == 1:\n        print(f\"epoch {ep:3d} | loss {loss:.4f}\")\n\nz1 = Xte @ W1.T + b1\nh1 = np.tanh(z1)\nU = h1 @ W2.T + b2\nUmax = U.max(axis=1, keepdims=True)\nex = np.exp(U - Umax)\nyhat = ex / ex.sum(axis=1, keepdims=True)\nypred = np.argmax(yhat, axis=1)\nacc = (ypred == yte).mean()\nprint(f\"\\nTest accuracy (3 classes): {acc:.3f}\")\n\n\n\n```\n</code></pre>"},{"location":"MLP/#result_2","title":"Result:","text":"<pre><code>```bash\nepoch   1 | loss 0.8970\nepoch  50 | loss 0.5367\nepoch 100 | loss 0.3963\nepoch 150 | loss 0.3406\nepoch 200 | loss 0.3817\nepoch 250 | loss 0.3475\nepoch 300 | loss 0.2155\n\nTest accuracy (3 classes): 0.817\n```\n</code></pre>"},{"location":"MLP/#exercicio-4","title":"Exerc\u00edcio 4","text":""},{"location":"MLP/#a-brief-description-of-my-approach_3","title":"A brief description of my approach","text":"<p>Repito o cen\u00e1rio do Ex. 3, mas aprofundo o modelo para 4\u201364\u201332\u20133, adicionando uma segunda camada escondida. Mantenho a implementa\u00e7\u00e3o inline e os mesmos cuidados num\u00e9ricos (softmax estabilizado + CE), comparando a acur\u00e1cia de teste com a vers\u00e3o menos profunda para observar o impacto da profundidade sob uma rotina de treino e backprop id\u00eantica.</p>"},{"location":"MLP/#code_3","title":"Code:","text":"<pre><code>```python\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nrng = np.random.default_rng(321)\n\n\nn_total = 1500\nn_per = n_total // 3\n\n\nsizes0 = [n_per//2, n_per - n_per//2]\nX0_parts = []\nfor k, ns in enumerate(sizes0):\n    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n                                n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n                                flip_y=0.0, random_state=70 + k)\n    yk[:] = 0\n    X0_parts.append((Xk, yk))\nX0 = np.vstack([p[0] for p in X0_parts]); y0 = np.hstack([p[1] for p in X0_parts])\n\n\nsizes1 = [n_per//3, n_per//3, n_per - 2*(n_per//3)]\nX1_parts = []\nfor k, ns in enumerate(sizes1):\n    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n                                n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n                                flip_y=0.0, random_state=80 + k)\n    yk[:] = 1\n    X1_parts.append((Xk, yk))\nX1 = np.vstack([p[0] for p in X1_parts]); y1 = np.hstack([p[1] for p in X1_parts])\n\n\nsizes2 = [n_per//4, n_per//4, n_per//4, n_per - 3*(n_per//4)]\nX2_parts = []\nfor k, ns in enumerate(sizes2):\n    Xk, yk = make_classification(n_samples=ns, n_features=4, n_informative=4, n_redundant=0,\n                                n_classes=2, n_clusters_per_class=1, class_sep=1.5,\n                                flip_y=0.0, random_state=90 + k)\n    yk[:] = 2\n    X2_parts.append((Xk, yk))\nX2 = np.vstack([p[0] for p in X2_parts]); y2 = np.hstack([p[1] for p in X2_parts])\n\nX = np.vstack([X0, X1, X2]).astype(float)\ny = np.hstack([y0, y1, y2]).astype(int)\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\nC = 3\nytr_oh = np.eye(C)[ytr]\n\nin_features = 4\nh1_size = 64\nh2_size = 32\nout_features = 3\n\nW1 = rng.normal(0, 1, (h1_size, in_features)) / np.sqrt(in_features)\nb1 = np.zeros((h1_size,))\n\nW2 = rng.normal(0, 1, (h2_size, h1_size)) / np.sqrt(h1_size)\nb2 = np.zeros((h2_size,))\n\nW3 = rng.normal(0, 1, (out_features, h2_size)) / np.sqrt(h2_size)\nb3 = np.zeros((out_features,))\n\nlr = 3e-2\nepochs = 350\nbatch_size = 64\neps = 1e-12\n\nN = Xtr.shape[0]\n\nfor ep in range(1, epochs+1):\n    idx = rng.permutation(N)\n    Xs = Xtr[idx]; ys = ytr_oh[idx]\n\n    for i in range(0, N, batch_size):\n        xb = Xs[i:i+batch_size]\n        yb = ys[i:i+batch_size]\n\n\n        z1 = xb @ W1.T + b1\n        h1 = np.tanh(z1)\n\n        z2 = h1 @ W2.T + b2\n        h2 = np.tanh(z2)\n\n        U = h2 @ W3.T + b3 \n        Umax = U.max(axis=1, keepdims=True)\n        ex = np.exp(U - Umax)\n        yhat = ex / ex.sum(axis=1, keepdims=True)\n\n\n        yclip = np.clip(yhat, eps, 1-eps)\n        loss = -np.mean(np.sum(yb * np.log(yclip), axis=1))\n\n\n        B = yb.shape[0]\n        dL_dU = (yhat - yb) / B  \n\n        dL_dW3 = dL_dU.T @ h2\n        dL_db3 = dL_dU.sum(axis=0)\n\n        dL_dh2 = dL_dU @ W3\n        dL_dz2 = dL_dh2 * (1 - np.tanh(z2)**2)\n\n        dL_dW2 = dL_dz2.T @ h1\n        dL_db2 = dL_dz2.sum(axis=0)\n\n        dL_dh1 = dL_dz2 @ W2\n        dL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n\n        dL_dW1 = dL_dz1.T @ xb\n        dL_db1 = dL_dz1.sum(axis=0)\n\n        W3 -= lr * dL_dW3\n        b3 -= lr * dL_db3\n        W2 -= lr * dL_dW2\n        b2 -= lr * dL_db2\n        W1 -= lr * dL_dW1\n        b1 -= lr * dL_db1\n\n    if ep % 50 == 0 or ep == 1:\n        print(f\"epoch {ep:3d} | loss {loss:.4f}\")\n\n\nz1 = Xte @ W1.T + b1\nh1 = np.tanh(z1)\nz2 = h1 @ W2.T + b2\nh2 = np.tanh(z2)\nU = h2 @ W3.T + b3\nUmax = U.max(axis=1, keepdims=True)\nex = np.exp(U - Umax)\nyhat = ex / ex.sum(axis=1, keepdims=True)\nypred = np.argmax(yhat, axis=1)\nacc = (ypred == yte).mean()\nprint(f\"\\nTest accuracy (deeper, 2 hidden): {acc:.3f}\")\n\n\n```\n</code></pre>"},{"location":"MLP/#result_3","title":"Result:","text":"<pre><code>```bash\nepoch   1 | loss 0.9812\nepoch  50 | loss 0.7336\nepoch 100 | loss 0.6125\nepoch 150 | loss 0.5455\nepoch 200 | loss 0.3711\nepoch 250 | loss 0.4223\nepoch 300 | loss 0.4229\nepoch 350 | loss 0.4302\n\nTest accuracy (deeper, 2 hidden): 0.700\n```\n</code></pre>"},{"location":"data/","title":"Atividade: 1. Data","text":""},{"location":"data/#exercicio-1-exploring-class-separability-in-2d","title":"Exerc\u00edcio 1 \u2014 Exploring Class Separability in 2D","text":""},{"location":"data/#a-brief-description-of-my-approach","title":"A brief description of my approach","text":"<p>Para gerar os dados, utilizo distribui\u00e7\u00f5es normais multivariadas com a fun\u00e7\u00e3o random.multivariate_normal do numpy, cada um com m\u00e9dia e covari\u00e2ncia diferentes. Cada ponto recebe um r\u00f3tulo de classe correspondente ao cluster de origem.</p>"},{"location":"data/#generate-the-data","title":"Generate  the data:","text":"<pre><code>data = {'x': [], 'y':[], 'class':[], 'color': []}\n\n\nmean_std_dev = [([2,3], np.diag([0.8, 2.5])) ,\n([5,6], np.diag([1.2, 1.9])),\n([8,1], np.diag([0.9, 0.9])),\n([15,4], np.diag([0.5, 2.0]))]\nmean = [2, 3]\ncov = np.diag([0.8, 2.5])\nfor i in range(0, 4):\n\n    mean = mean_std_dev[i][0]\n    cov = mean_std_dev[i][1]\n\n    x, y = np.random.multivariate_normal(mean, cov, 100).T\n    classe = []\n    for c in range(0, len(x)):\n        data['x'].append(x[c])\n        data['y'].append(y[c])\n        data['class'].append(f\"class_{i}\")\n        if i == 0:\n            data['color'].append((1.0, 0.0, 0.0))\n        if i == 1:\n            data['color'].append((1.0,0.0,1.0,))\n        if i == 2:\n            data['color'].append((1.0, 1.0, 0.0))\n        if i == 3:\n            data['color'].append((1.0, 0.5, 0.5))\n</code></pre>"},{"location":"data/#ploting-the-data","title":"Ploting the Data","text":""},{"location":"data/#analyze-and-draw-boundaries","title":"Analyze and Draw Boundaries","text":"<p>a .No gr\u00e1fico de dispers\u00e3o, observa-se que as duas classes mais \u00e0 direita (rosa claro) est\u00e3o bem separadas das demais, sem sobreposi\u00e7\u00e3o aparente. J\u00e1 as duas classes mais \u00e0 esquerda (vermelha e roxa) apresentam consider\u00e1vel sobreposi\u00e7\u00e3o em suas regi\u00f5es, tornando dif\u00edcil distingui-las linearmente. A classe amarela encontra-se abaixo, relativamente bem delimitada, embora mais pr\u00f3xima das classes vermelha e roxa.</p> <p>b. Observando o plot, seriam necess\u00e1rias tr\u00eas fronteiras lineares para separar corretamente todas as classes. Com apenas uma, seria poss\u00edvel dividir em metades (esquerda e direita), mas cada lado ainda conteria duas classes. Com duas fronteiras, ainda restaria um lado com duas classes agrupadas. Assim, somente com uma terceira linha seria poss\u00edvel separar essas duas classes restantes.</p> <p>c.</p>"},{"location":"data/#_1","title":"1.Data","text":""},{"location":"data/#exercicio-2-non-linearity-in-higher-dimensions","title":"Exerc\u00edcio 2 \u2014 Non-Linearity in Higher Dimensions","text":""},{"location":"data/#a-brief-description-of-my-approach_1","title":"A brief description of my approach","text":"<p>Para geras os dados em cinco dimens\u00f5es foi utilizados distribui\u00e7\u00f5es normais multivariadas, cada um com atributos diferentes. Cada amostra recebeu um rotulo de classe (A ou B), dependendo da origem dela.</p>"},{"location":"data/#generate-the-data_1","title":"Generate  the data:","text":"<pre><code>import numpy as np\nimport pandas as pd\n\nrng = np.random.default_rng(42)\ndata = {\n    'x1': [], 'x2': [], 'x3': [], 'x4': [], 'x5': [],\n    'class': [], 'color': []\n}\nparams = [\n    (\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n        np.array([\n            [1.0, 0.8, 0.1, 0.0, 0.0],\n            [0.8, 1.0, 0.3, 0.0, 0.0],\n            [0.1, 0.3, 1.0, 0.5, 0.0],\n            [0.0, 0.0, 0.5, 1.0, 0.2],\n            [0.0, 0.0, 0.0, 0.2, 1.0],\n        ]),\n        \"class_A\",\n        (1.0, 0.0, 0.0), \n    ),\n    (\n        np.array([1.5, 1.5, 1.5, 1.5, 1.5]),\n        np.array([\n            [1.5, -0.7, 0.2, 0.0, 0.0],\n            [-0.7, 1.5, 0.4, 0.0, 0.0],\n            [0.2, 0.4, 1.5, 0.6, 0.0],\n            [0.0, 0.0, 0.6, 1.5, 0.3],\n            [0.0, 0.0, 0.0, 0.3, 1.5],\n        ]),\n        \"class_B\",\n        (0.0, 0.0, 1.0), \n    ),\n]\n\nn_per_class = 500\n\nfor mean, cov, label, color in params:\n    samples = rng.multivariate_normal(mean, cov, size=n_per_class)\n    for s in samples:\n        data['x1'].append(s[0])\n        data['x2'].append(s[1])\n        data['x3'].append(s[2])\n        data['x4'].append(s[3])\n        data['x5'].append(s[4])\n        data['class'].append(label)\n        data['color'].append(color)\n\n\ndf = pd.DataFrame(data)\n</code></pre>"},{"location":"data/#visualize-the-data","title":"Visualize the data","text":""},{"location":"data/#analyze-the-plot","title":"Analyze the plot","text":"<p>a. Observando o gr\u00e1fico, percebe-se que os pontos vermelhos tendem a se concentrar mais \u00e0 esquerda, enquanto os azuis ficam predominantemente \u00e0 direita. No entanto, h\u00e1 uma sobreposi\u00e7\u00e3o consider\u00e1vel: v\u00e1rios pontos vermelhos aparecem em regi\u00f5es azuis e vice-versa. Isso significa que uma simples linha no meio do gr\u00e1fico n\u00e3o conseguiria separar as classes com boa acur\u00e1cia, j\u00e1 que h\u00e1 mistura entre elas.</p> <p>b. Esse tipo de dado imp\u00f5e um desafio, pois um modelo linear teria dificuldade em capturar esses casos de sobreposi\u00e7\u00e3o. Seriam necess\u00e1rias m\u00faltiplas fronteiras lineares para tentar contornar o problema, o que aumenta o risco de overfitting. Por isso, uma rede neural com m\u00faltiplas camadas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares se mostra mais adequada, j\u00e1 que consegue modelar fronteiras de decis\u00e3o complexas e lidar melhor com regi\u00f5es de interse\u00e7\u00e3o entre as classes.</p>"},{"location":"data/#exercicio-3-preparing-real-world-data-for-a-neural-network","title":"Exerc\u00edcio 3 \u2014 Preparing Real-World Data for a Neural Network","text":""},{"location":"data/#a-brief-description-of-my-approach_2","title":"A brief description of my approach","text":"<p>Carrego o dataset e realizo o tratamento dos valores faltantes, usando a mediana para vari\u00e1veis num\u00e9ricas, a moda para vari\u00e1veis bin\u00e1rias e preenchendo as categ\u00f3ricas com \"Unknown\".</p> <p>Em seguida, aplico one-hot encoding nas vari\u00e1veis categ\u00f3ricas e normalizo os atributos num\u00e9ricos para a faixa [-1, 1].</p>"},{"location":"data/#describe-the-data","title":"Describe the Data","text":"<ul> <li> <p>O objetivo do dataset \"Spaceship Titanic\" \u00e9 prever se os passageiros foram transportados para outra dimens\u00e3o. A coluna \"Transported\" representa esse resultado: valor 1 indica que o passageiro foi transportado e valor 0 indica que n\u00e3o foi.</p> </li> <li> <p>Features num\u00e9ricas: Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck.</p> </li> <li> <p>Features categ\u00f3ricas: PassengerId, HomePlanet, CryoSleep, Cabin, Destination, VIP, Name.</p> </li> <li> <p>Valores faltantes por coluna:</p> </li> </ul> Coluna N\u00ba de faltantes CryoSleep 217 ShoppingMall 208 VIP 203 HomePlanet 201 Name 200 Cabin 199 VRDeck 188 FoodCourt 183 Spa 183 Destination 182 RoomService 181 Age 179"},{"location":"data/#preprocess-the-data","title":"Preprocess the Data","text":"<pre><code>def report_missing(df):\n    miss = df.isna().sum()\n    miss = miss[miss &gt; 0].sort_values(ascending=False)\n    print(\"\\nFaltantes por coluna:\\n\" + (miss.to_string() if len(miss) else \"Nenhum\"))\n    return miss\n\ndef one_hot_manual(series, prefix=None):\n    s = series.astype(\"category\")\n    cats = list(s.cat.categories)\n    out = pd.DataFrame(index=series.index)\n    for c in cats:\n        col = f\"{prefix or series.name}__{c}\"\n        out[col] = (s == c).astype(int)\n    return out\n\ndef minmax_pm1_df(df, cols):\n    mn = df[cols].min(axis=0)\n    mx = df[cols].max(axis=0)\n    denom = (mx - mn).replace(0, 1.0)\n    scaled = 2 * ((df[cols] - mn) / denom) - 1\n    return scaled, mn, mx\n\n\ndef prepare_spaceship_norm(path=CSV_PATH):\n    df = pd.read_csv(path)\n    print(\"Carregado:\", path, \"shape:\", df.shape)\n\n    num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n    bin_cols = [\"CryoSleep\", \"VIP\"]           \n    cat_cols = [\"HomePlanet\", \"Destination\"]   \n    target = \"Transported\"\n    for c in num_cols:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    y = df[target].astype(int)\n\n    report_missing(df)\n    for c in num_cols:\n        df[c] = df[c].fillna(df[c].median())\n    for c in bin_cols:\n        m = df[c].mode(dropna=True)\n        fill = m.iloc[0] if len(m) else False\n        df[c] = df[c].fillna(fill).map({True: 1, False: 0}).astype(int)\n    for c in cat_cols:\n        df[c] = df[c].fillna(\"Unknown\").astype(str)\n\n    oh_parts = [one_hot_manual(df[c], prefix=c) for c in cat_cols]\n    X_cat = pd.concat(oh_parts, axis=1)\n    X_num_scaled, mn, mx = minmax_pm1_df(df, num_cols)\n\n    X = pd.concat([X_num_scaled[num_cols], df[bin_cols].astype(int), X_cat], axis=1)\n    print(\"Features finais:\", X.shape)\n\n    plt.figure(figsize=(6,4)); df[\"Age\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"Age \u2014 ANTES (valor bruto)\"); plt.xlabel(\"Age\"); plt.tight_layout()\n\n    plt.figure(figsize=(6,4)); X_num_scaled[\"Age\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"Age \u2014 DEPOIS (normalizado [-1,1])\"); plt.xlabel(\"Age norm.\"); plt.tight_layout()\n\n    plt.figure(figsize=(6,4)); df[\"FoodCourt\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"FoodCourt \u2014 ANTES (valor bruto)\"); plt.xlabel(\"Gasto\"); plt.tight_layout()\n\n    plt.figure(figsize=(6,4)); X_num_scaled[\"FoodCourt\"].plot.hist(bins=30, alpha=0.85)\n    plt.title(\"FoodCourt \u2014 DEPOIS (normalizado [-1,1])\"); plt.xlabel(\"Gasto norm.\"); plt.tight_layout()\n\n    plt.show()\n    return X, y, {\"min\": mn, \"max\": mx}\n</code></pre>"},{"location":"data/#visualize-the-results","title":"Visualize the Results","text":""},{"location":"data/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Descri\u00e7\u00e3o do Spaceship Titanic e do alvo <code>Transported</code>: https://www.kaggle.com/competitions/spaceship-titanic/data.  </li> </ul>"},{"location":"perceptron/","title":"Atividade: 2. Perceptron","text":""},{"location":"perceptron/#exercicio-1","title":"Exerc\u00edcio 1","text":""},{"location":"perceptron/#a-brief-description-of-my-approach","title":"A brief description of my approach","text":"<p>Para gerar os dados utilizo a fun\u00e7\u00e3o <code>multivariate_normal</code>, que permite passar o vetor de m\u00e9dias e a matriz de covari\u00e2ncia para aplicar a distribui\u00e7\u00e3o <code>Gaussiana</code> multivariada. Em seguida, aplico um vstack para formar o vetor X, empilhando os arrays verticalmente, enquanto para o vetor y utilizo um hstack, que os empilha horizontalmente.</p> <p>Na implementa\u00e7\u00e3o do Perceptron, crio um loop externo para limitar o n\u00famero m\u00e1ximo de \u00e9pocas, caso n\u00e3o ocorra a converg\u00eancia. Dentro desse loop, antes de entrar no interno, inicializo as vari\u00e1veis updates e correct com zero. No loop interno, verifico se ocorre algum erro; em caso positivo, recalculo os pesos e o vi\u00e9s. Caso n\u00e3o ocorra nenhum erro durante toda a \u00e9poca, significa que houve converg\u00eancia, e o loop \u00e9 interrompido.</p>"},{"location":"perceptron/#generate-the-data","title":"Generate  the data:","text":"<pre><code>mean_class0 = np.array([1.5, 1.5])\nmean_class1 = np.array([5.0, 5.0])\ncov   = np.array([[0.5, 0.0],[0.0, 0.5]])\nX_class0 = rng.multivariate_normal(mean_class0, cov, size=samples_per_class)\nX_class1 = rng.multivariate_normal(mean_class1, cov, size=samples_per_class)\nX  = np.vstack([X_class0, X_class1])\ny  = np.hstack([np.zeros(samples_per_class, dtype=int), np.ones(samples_per_class,  dtype=int)])\n</code></pre>"},{"location":"perceptron/#ploting-the-data","title":"Ploting the Data","text":""},{"location":"perceptron/#perceptron-implementation-task","title":"Perceptron Implementation Task:","text":"<p><pre><code>r=0.01 \nmax_epochs=100 \n\n\nrng = np.random.default_rng(seed)\nn, d = X.shape\nw = np.zeros(d)\nb = 0.0\nacc_hist = []\n\nfor epoch in range(1, max_epochs+1):\n    idx = np.arange(n)\n\n    rng.shuffle(idx)\n    updates = 0\n    correct = 0\n\n    for i in idx:\n        z = np.dot(w, X[i]) + b\n        y_hat = 1 if z &gt;= 0 else 0\n        e = y[i] - y_hat \n        if e != 0:\n\n            w = w + lr * e * X[i]\n            b = b + lr * e\n            updates += 1\n        else:\n            correct += 1\n\n        acc = correct / n\n        acc_hist.append(acc)\n\n        if updates == 0:\n            break\n</code></pre> </p>"},{"location":"perceptron/#reporting-results","title":"Reporting results","text":"<ul> <li>Pesos finais w: [0.04894919 0.06400311]</li> <li>Vi\u00e9s final b: -0.3200000000000001</li> <li>acur\u00e1cia final: 99.60%</li> </ul> <p>Para entender o motivo da converg\u00eancia r\u00e1pida, \u00e9 necess\u00e1rio explicar que o Perceptron funciona com uma regra de atualiza\u00e7\u00e3o: sempre que um ponto \u00e9 classificado incorretamente, os pesos s\u00e3o ajustados na dire\u00e7\u00e3o correta para reduzir o erro. Neste caso, como h\u00e1 uma separabilidade linear clara entre as classes, o algoritmo atinge a converg\u00eancia rapidamente, pois n\u00e3o h\u00e1 necessidade de muitas atualiza\u00e7\u00f5es para encontrar uma fronteira de decis\u00e3o adequada.</p> <p></p>"},{"location":"perceptron/#exercicio-2","title":"Exerc\u00edcio 2","text":""},{"location":"perceptron/#a-brief-description-of-my-approach_1","title":"A brief description of my approach","text":"<p>Neste exerc\u00edcio os dados s\u00e3o gerados da mesma forma que no primeiro, mas o Perceptron \u00e9 treinado em 5 execu\u00e7\u00f5es com inicializa\u00e7\u00f5es diferentes. Para cada execu\u00e7\u00e3o registro a acur\u00e1cia por \u00e9poca e, ao final, calculo a m\u00e9dia e o desvio-padr\u00e3o das acur\u00e1cias, alinhando os hist\u00f3ricos quando necess\u00e1rio. Isso permite avaliar o desempenho m\u00e9dio do modelo em dados com sobreposi\u00e7\u00e3o e reduzir a influ\u00eancia da inicializa\u00e7\u00e3o aleat\u00f3ria.</p>"},{"location":"perceptron/#data-generation-task","title":"Data Generation Task:","text":"<pre><code>mean_class0 = np.array([3.0, 3.0])\nmean_class1 = np.array([4.0, 4.0])\ncov   = np.array([[1.5, 0.0],[0.0, 1.5]])\nX_class0 = rng.multivariate_normal(mean_class0, cov, size=samples_per_class)\nX_class1 = rng.multivariate_normal(mean_class1, cov, size=samples_per_class)\nX  = np.vstack([X_class0, X_class1])\ny  = np.hstack([np.zeros(samples_per_class, dtype=int), np.ones(samples_per_class,  dtype=int)])\n</code></pre>"},{"location":"perceptron/#visualize-the-data","title":"Visualize the data","text":""},{"location":"perceptron/#perceptron-implementation-task_1","title":"Perceptron implementation task","text":"<pre><code>num_runs = 5\nall_hist = []\nfinal_accs = []\nfinal_ws = []\nfinal_bs = []\n\nfor run in range(num_runs):\n    local_rng = np.random.default_rng(100 + run)\n\n    w_r = local_rng.normal(0, 0.01, size=X.shape[1])\n    b_r = 0.0\n\n    acc_hist_r = []\n    idx = np.arange(y.shape[0])\n\n    for epoch in range(1, 101):  \n        local_rng.shuffle(idx)\n        updates = 0\n        correct = 0\n\n        for i in idx:\n            xi = X[i]\n            yi = y[i]\n            z  = float(np.dot(w_r, xi) + b_r)\n            yhat = 1 if z &gt;= 0 else 0\n            err  = int(yi - yhat)\n\n            if err != 0:\n                w_r = w_r + 0.01 * err * xi\n                b_r = b_r + 0.01 * err\n                updates += 1\n            else:\n                correct += 1\n\n        acc = correct / len(y)\n        acc_hist_r.append(acc)\n\n        if updates == 0:  \n            break\n\n\n    all_hist.append(acc_hist_r)\n    y_pred_r = ((X @ w_r + b_r) &gt;= 0).astype(int)\n    final_accs.append((y_pred_r == y).mean())\n    final_ws.append(w_r.copy())\n    final_bs.append(b_r)\n\n\nmax_len = max(len(h) for h in all_hist)\naligned = []\nfor h in all_hist:\n    if len(h) &lt; max_len:\n        h = h + [h[-1]] * (max_len - len(h))\n    aligned.append(h)\n\naligned = np.array(aligned)\nmean_acc = aligned.mean(axis=0)\nstd_acc  = aligned.std(axis=0)\n\n\nfor i in range(num_runs):\n    print(f\"Run {i+1}:\")\n    print(f\"  Final weights: {final_ws[i]}\")\n    print(f\"  Final bias: {final_bs[i]}\")\n    print(f\"  Final accuracy: {final_accs[i]*100:.2f}%\")\n    print()\n\nprint(\"Summary across runs:\")\nprint(\"  Final accuracies:\", [f\"{a*100:.2f}%\" for a in final_accs])\nprint(f\"  Mean final accuracy: {np.mean(final_accs)*100:.2f}%\")\nprint(f\"  Std final accuracy: {np.std(final_accs)*100:.2f}%\")\n</code></pre>"},{"location":"perceptron/#reporting-results_1","title":"Reporting results","text":"<p>Run 1:   Final weights: [-0.02059995  0.1328929 ]   Final bias: -0.43000000000000016   Final accuracy: 63.00%</p> <p>Run 2:   Final weights: [0.05533817 0.05429294]   Final bias: -0.42000000000000015   Final accuracy: 69.65%</p> <p>Run 3:   Final weights: [0.05812441 0.09586418]   Final bias: -0.41000000000000014   Final accuracy: 64.50%</p> <p>Run 4:   Final weights: [0.00919726 0.05628983]   Final bias: -0.43000000000000016   Final accuracy: 50.45%</p> <p>Run 5:   Final weights: [0.13061734 0.03119603]   Final bias: -0.41000000000000014   Final accuracy: 60.85%</p> <p>Summary across runs:   Final accuracies: ['63.00%', '69.65%', '64.50%', '50.45%', '60.85%']   Mean final accuracy: 61.69%   Std final accuracy: 6.32%</p> <p></p> <p>Esses dados, diferentemente do primeiro caso, n\u00e3o apresentam uma separabilidade linear clara, como pode ser observado na imagem. Isso torna dif\u00edcil para o Perceptron alcan\u00e7ar a converg\u00eancia, pois sempre haver\u00e1 pontos que n\u00e3o podem ser separados corretamente por uma \u00fanica reta. Assim, o algoritmo permanece ajustando os pesos e o vi\u00e9s a cada \u00e9poca, sem conseguir estabilizar em uma solu\u00e7\u00e3o perfeita.</p>"},{"location":"vae/","title":"4.VAE","text":""},{"location":"vae/#a-brief-description-of-my-approach","title":"A brief description of my approach","text":"<p>Implementei um Variational Autoencoder (VAE) em TensorFlow/Keras para o MNIST, usando arquitetura MLP com LATENT_DIM = 2 e camadas ocultas [512, 256]. O pipeline usa <code>tf.data</code> com <code>batch=128</code>, 20 \u00e9pocas, <code>Adam(lr=1e-3)</code> .</p>"},{"location":"vae/#1-dados-e-tfdata","title":"1. Dados e <code>tf.data</code>","text":"<p>Carrego o MNIST do Keras, normalizo para <code>[0,1]</code>, adiciono canal e crio <code>Dataset</code> batched/prefetched: <pre><code>def load_dataset(name=\"MNIST\"):\n\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    x_train = (x_train.astype(\"float32\") / 255.0)[..., None]\n    x_test  = (x_test.astype(\"float32\")  / 255.0)[..., None]\n    return (x_train, y_train), (x_test, y_test)\n\n(x_train, y_train), (x_test, y_test) = load_dataset(DATASET)\n\nn_total = x_train.shape[0]\nn_val = int(n_total * VAL_SPLIT)\nx_val, y_val = x_train[:n_val], y_train[:n_val]\nx_train, y_train = x_train[n_val:], y_train[n_val:]\n\ndef make_ds(x, y, shuffle=True):\n    ds = tf.data.Dataset.from_tensor_slices((x, y))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(x), seed=SEED, reshuffle_each_iteration=True)\n    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return ds\n\ntrain_ds = make_ds(x_train, y_train, shuffle=True)\nval_ds   = make_ds(x_val, y_val, shuffle=False)\ntest_ds  = make_ds(x_test, y_test, shuffle=False)\n</code></pre></p>"},{"location":"vae/#2encoder-keras-model","title":"2.Encoder (Keras Model)","text":"<p>O encoder achata a imagem e produz mu e logvar via MLP: <pre><code>class Encoder(keras.Model):\n    def __init__(self, hidden_dims=HIDDEN_DIMS, latent_dim=LATENT_DIM):\n        super().__init__()\n        self.flatten = layers.Flatten()\n        mlp = []\n        dims = [INPUT_DIM] + hidden_dims\n        for i in range(len(dims) - 1):\n            mlp += [layers.Dense(dims[i + 1], activation=\"relu\")]\n        self.mlp = keras.Sequential(mlp)\n        self.mu = layers.Dense(latent_dim)\n        self.logvar = layers.Dense(latent_dim)\n\n    def call(self, x, training=False):\n        x = self.flatten(x)\n        h = self.mlp(x, training=training)\n        mu = self.mu(h)\n        logvar = self.logvar(h)\n        return mu, logvar\n</code></pre></p>"},{"location":"vae/#3reparametrizacao","title":"3.Reparametriza\u00e7\u00e3o","text":"<p>Reparametriza\u00e7\u00e3o para manter o gradiente: <pre><code>def reparameterize(self, mu, logvar):\n        eps = tf.random.normal(shape=tf.shape(mu))\n        std = tf.exp(0.5 * logvar)\n        return mu + std * eps\n</code></pre></p>"},{"location":"vae/#4decoder","title":"4.Decoder","text":"<p>Combino BCE (reconstru\u00e7\u00e3o) com KL (regulariza\u00e7\u00e3o do espa\u00e7o latente para normal padr\u00e3o). <pre><code>class Decoder(keras.Model):\n    def __init__(self, hidden_dims=HIDDEN_DIMS, latent_dim=LATENT_DIM, output_dim=INPUT_DIM):\n        super().__init__()\n        mlp = []\n        dims = [latent_dim] + hidden_dims[::-1]\n        for i in range(len(dims) - 1):\n            mlp += [layers.Dense(dims[i + 1], activation=\"relu\")]\n        mlp += [layers.Dense(output_dim, activation=\"sigmoid\")]\n        self.mlp = keras.Sequential(mlp)\n        self.reshape = layers.Reshape(INPUT_SHAPE)\n\n    def call(self, z, training=False):\n        x_hat_flat = self.mlp(z, training=training)\n        x_hat = self.reshape(x_hat_flat)\n        return x_hat\n</code></pre></p>"},{"location":"vae/#5-vae-com-train_steptest_step-customizados","title":"5. VAE com train_step/test_step customizados","text":"<p>Combino BCE de reconstru\u00e7\u00e3o com KL por amostra. A classe sobrescreve train_step/test_step para registrar m\u00e9tricas:</p> <pre><code>class VAE(keras.Model):\n    def __init__(self, hidden_dims=HIDDEN_DIMS, latent_dim=LATENT_DIM):\n        super().__init__()\n        self.encoder = Encoder(hidden_dims, latent_dim)\n        self.decoder = Decoder(hidden_dims, latent_dim)\n        self.total_loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.recon_loss_tracker = keras.metrics.Mean(name=\"recon\")\n        self.kl_loss_tracker    = keras.metrics.Mean(name=\"kl\")\n\n    def reparameterize(self, mu, logvar):\n        eps = tf.random.normal(shape=tf.shape(mu))\n        std = tf.exp(0.5 * logvar)\n        return mu + std * eps\n\n    def compute_loss(self, x, training=False):\n        mu, logvar = self.encoder(x, training=training)\n        z = self.reparameterize(mu, logvar)\n        x_hat = self.decoder(z, training=training)\n\n\n        bce_per_pixel = keras.losses.binary_crossentropy(x, x_hat)\n        bce_per_sample = tf.reduce_sum(bce_per_pixel, axis=[1, 2])\n        recon_loss = tf.reduce_mean(bce_per_sample)\n\n        kl_per_sample = -0.5 * tf.reduce_sum(\n            1.0 + logvar - tf.square(mu) - tf.exp(logvar), axis=1\n        )\n        kl_loss = tf.reduce_mean(kl_per_sample)\n\n        total = recon_loss + kl_loss\n        return total, recon_loss, kl_loss, x_hat, mu, logvar, z\n\n\n    def call(self, x, training=False):\n        mu, logvar = self.encoder(x, training=training)\n        z = self.reparameterize(mu, logvar)\n        x_hat = self.decoder(z, training=training)\n        return x_hat\n\n    def train_step(self, data):\n        x, _ = data\n        with tf.GradientTape() as tape:\n            total, recon, kl, _, _, _, _ = self.compute_loss(x, training=True)\n        grads = tape.gradient(total, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n\n        self.total_loss_tracker.update_state(total)\n        self.recon_loss_tracker.update_state(recon)\n        self.kl_loss_tracker.update_state(kl)\n        return {\"loss\": self.total_loss_tracker.result(),\n                \"recon\": self.recon_loss_tracker.result(),\n                \"kl\": self.kl_loss_tracker.result()}\n\n    def test_step(self, data):\n        x, _ = data\n        total, recon, kl, _, _, _, _ = self.compute_loss(x, training=False)\n        self.total_loss_tracker.update_state(total)\n        self.recon_loss_tracker.update_state(recon)\n        self.kl_loss_tracker.update_state(kl)\n        return {\"loss\": self.total_loss_tracker.result(),\n                \"recon\": self.recon_loss_tracker.result(),\n                \"kl\": self.kl_loss_tracker.result()}\n</code></pre>"},{"location":"vae/#6-treinamento-com-checkpoint-do-melhor-modelo","title":"6. Treinamento com checkpoint do \u201cmelhor\u201d modelo","text":"<p>Compilo com Adam, treino 20 \u00e9pocas e salvo apenas os pesos com melhor val_loss: <pre><code>vae = VAE()\noptimizer = keras.optimizers.Adam(LR)\nvae.compile(optimizer=optimizer)\n\n\n_ = vae(tf.zeros((1, *INPUT_SHAPE))) \n\n\nckpt_path = \"vae_best_tf.weights.h5\"\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    ckpt_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True\n)\n\nhistory = vae.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    callbacks=[checkpoint_cb],\n    verbose=1,\n)\n\n\nbest_vae = VAE()\nbest_vae.compile(optimizer=keras.optimizers.Adam(LR))\n_ = best_vae(tf.zeros((1, *INPUT_SHAPE)))  \nbest_vae.load_weights(ckpt_path)\n</code></pre></p>"},{"location":"vae/#7-avaliacao-reconstrucoes-amostras-e-espaco-latente","title":"7. Avalia\u00e7\u00e3o: reconstru\u00e7\u00f5es, amostras e espa\u00e7o latente","text":"<p>Gero figuras de reconstru\u00e7\u00f5es (original \u00d7 reconstru\u00edda), amostras aleat\u00f3rias (z ~ N(0, I)) e dispers\u00e3o do espa\u00e7o latente pelas m\u00e9dias \u03bc:</p> <pre><code>def plot_reconstructions(model, dataset, max_images=16, title=\"Reconstru\u00e7\u00f5es\"):\n    x_batch, _ = next(iter(dataset))\n    x = x_batch[:max_images]\n    mu, logvar = model.encoder(x, training=False)\n    z = model.reparameterize(mu, logvar)\n    x_hat = model.decoder(z, training=False)\n    x = x.numpy()\n    x_hat = x_hat.numpy()\n    fig, axes = plt.subplots(2, max_images, figsize=(1.5*max_images, 3))\n    for i in range(max_images):\n        axes[0, i].imshow(x[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n        axes[0, i].axis(\"off\")\n        axes[1, i].imshow(x_hat[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n        axes[1, i].axis(\"off\")\n    fig.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_random_samples(model, n=SAMPLES_N, title=\"Random Samples\"):\n    z = tf.random.normal(shape=(n, LATENT_DIM))\n    x_hat = model.decoder(z, training=False).numpy()\n    cols = int(math.sqrt(n))\n    rows = int(math.ceil(n / cols))\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*1.5, rows*1.5))\n    axes = np.array(axes).reshape(rows, cols)\n    idx = 0\n    for r in range(rows):\n        for c in range(cols):\n            if idx &lt; n:\n                axes[r, c].imshow(x_hat[idx].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n            axes[r, c].axis(\"off\")\n            idx += 1\n    fig.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_latent_space(model, dataset, max_points=5000):\n    mus, ys, seen = [], [], 0\n    for x_batch, y_batch in dataset:\n        mu, _ = model.encoder(x_batch, training=False)\n        mus.append(mu.numpy())\n        ys.append(y_batch.numpy())\n        seen += x_batch.shape[0]\n        if seen &gt;= max_points:\n            break\n    Z = np.concatenate(mus)[:max_points]\n    Y = np.concatenate(ys)[:max_points]\n    if Z.shape[1] &gt;= 2:\n        plt.figure(figsize=(7, 6))\n        plt.scatter(Z[:, 0], Z[:, 1], c=Y, s=6, alpha=0.7)\n        plt.colorbar(label=\"label\")\n        plt.title(\"Espa\u00e7o Latente (\u03bc)\")\n        plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"Latent dimension &lt; 2; pulando latent scatter.\")\n\n\nplot_reconstructions(best_vae, val_ds, max_images=16, title=\"Val Reconstructions (Best)\")\nplot_random_samples(best_vae, n=SAMPLES_N, title=\"Random Samples (Best)\")\nplot_latent_space(best_vae, test_ds, max_points=5000)\n</code></pre>"},{"location":"vae/#resultados","title":"Resultados","text":"<p>O modelo Variational Autoencoder (VAE) foi treinado no dataset MNIST com 20 \u00e9pocas, utilizando uma arquitetura totalmente conectada (MLP) e um espa\u00e7o latente bidimensional (<code>LATENT_DIM = 2</code>). O objetivo foi aprender uma representa\u00e7\u00e3o comprimida das imagens de d\u00edgitos e gerar novas amostras coerentes com os dados originais.</p> <p>Durante o treinamento, observou-se a converg\u00eancia est\u00e1vel das perdas BCE (reconstru\u00e7\u00e3o) e KL (regulariza\u00e7\u00e3o). A perda de reconstru\u00e7\u00e3o diminuiu consistentemente, indicando que o modelo aprendeu a reproduzir bem as entradas, enquanto a diverg\u00eancia KL estabilizou, refletindo o aprendizado de uma distribui\u00e7\u00e3o latente regularizada.</p>"},{"location":"vae/#reconstrucoes","title":"Reconstru\u00e7\u00f5es","text":"<p>As imagens abaixo mostram os d\u00edgitos originais (linha superior) e suas respectivas reconstru\u00e7\u00f5es (linha inferior) feitas pelo VAE ap\u00f3s o treinamento.</p> <p></p> <p>An\u00e1lise: As reconstru\u00e7\u00f5es s\u00e3o bastante fi\u00e9is aos d\u00edgitos originais. Pequenas regi\u00f5es borradas aparecem em d\u00edgitos mais complexos (como 3 e 8), o que \u00e9 esperado dado o uso de uma rede MLP simples e o baixo tamanho do espa\u00e7o latente (2D).</p>"},{"location":"vae/#amostras-aleatorias-do-espaco-latente","title":"Amostras Aleat\u00f3rias do Espa\u00e7o Latente","text":"<p>O modelo tamb\u00e9m foi avaliado em sua capacidade generativa, amostrando vetores <code>z ~ N(0, I)</code> e decodificando-os em imagens.</p> <p></p> <p>An\u00e1lise: As imagens sintetizadas s\u00e3o coerentes e diversas, reproduzindo diferentes d\u00edgitos com boa qualidade visual. Alguns exemplos apresentam leve borramento \u2014 um indicativo de sobreposi\u00e7\u00e3o de regi\u00f5es no espa\u00e7o latente, algo esperado com <code>LATENT_DIM = 2</code>.</p>"},{"location":"vae/#espaco-latente","title":"Espa\u00e7o Latente","text":"<p>A figura abaixo mostra o espa\u00e7o latente aprendido, utilizando as m\u00e9dias <code>\u03bc</code> geradas pelo encoder. Cada ponto representa uma imagem, colorido de acordo com sua classe (d\u00edgito de 0 a 9).</p> <p></p> <p>An\u00e1lise: O gr\u00e1fico evidencia que o VAE conseguiu organizar o espa\u00e7o latente de forma estruturada, separando diferentes classes em regi\u00f5es distintas. Mesmo com apenas duas dimens\u00f5es, \u00e9 poss\u00edvel observar clusters bem definidos (por exemplo, d\u00edgitos 0, 1 e 7 aparecem claramente isolados).</p>"},{"location":"vae/#desafios-e-aprendizados","title":"Desafios e Aprendizados","text":""},{"location":"vae/#desafios-enfrentados","title":"Desafios enfrentados","text":"<ul> <li>Compatibilidade de vers\u00f5es entre TensorFlow, NumPy e Python 3.12 durante a instala\u00e7\u00e3o.  </li> <li>Ajuste do espa\u00e7o latente (dimens\u00e3o 2): dimens\u00f5es muito pequenas reduzem a qualidade das reconstru\u00e7\u00f5es, mas s\u00e3o ideais para visualiza\u00e7\u00e3o.  </li> <li>Balancear as perdas BCE e KL: um valor alto de KL pode levar a amostras menos n\u00edtidas, enquanto valores baixos reduzem a diversidade gerada.</li> </ul>"},{"location":"vae/#insights-obtidos","title":"Insights obtidos","text":"<ul> <li>Mesmo com uma arquitetura simples (MLP), o VAE \u00e9 capaz de aprender distribui\u00e7\u00f5es latentes significativas e gerar novos exemplos realistas.  </li> <li>O espa\u00e7o latente 2D permite interpretabilidade visual: \u00e9 poss\u00edvel entender como os diferentes d\u00edgitos se distribuem e se relacionam.  </li> <li>A regulariza\u00e7\u00e3o KL \u00e9 fundamental para que o modelo aprenda uma distribui\u00e7\u00e3o cont\u00ednua e suave, essencial para a gera\u00e7\u00e3o de novas amostras.</li> </ul>"},{"location":"vae/#conclusao","title":"Conclus\u00e3o","text":"<p>O experimento demonstra o funcionamento pr\u00e1tico de um Variational Autoencoder, mostrando: - Reconstru\u00e7\u00e3o fiel das imagens originais, - Capacidade generativa realista, - Organiza\u00e7\u00e3o sem\u00e2ntica no espa\u00e7o latente.</p>"}]}